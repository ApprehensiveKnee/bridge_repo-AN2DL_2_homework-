{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------* FIRST ATTEMPT *---------------------------------------\n",
    "# Problem description: develop a forecasting model that is able to predict several uncorrelated time series\n",
    "\n",
    "# DATA STRUCTURE: \n",
    "# Single folder containing the following files:\n",
    "# -> 'training_data.npy': it contains a numpy array of shape (48000, 2776). 48000 time series of length 2776.\n",
    "# -> 'valid_periods.npy': it contains a numpy array of type (48000, 2) containing for each of the time series the start and end index of the current series, i.e. the part without padding.\n",
    "# -> 'categories.npy': it contains a numpy array of shape (48000,), containing for each of the time series the code of its category. The possible categories are in {'A', 'B', 'C', 'D', 'E', 'F'}.\n",
    "# IMPORTANT: This is a dataset consisting of monovariate time series, i.e. composed of a single feature, belonging to six different domains. The time series of each domain are not to be understood as closely related to each other, but only as collected from similar data sources.\n",
    "# What is required of you is therefore to build a model that is capable of generalising sufficiently to predict the future samples of the 60 time series of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and connect to drive personal folder\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/homework_2/Edoardo\n",
    "\n",
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Import other support libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for data inport:\n",
    "# The function takes as input the path of the folder containing the data and returns the training data, the validation periods and the categories\n",
    "# Before doing so, it eventually unzips it if the zip flag is set to True\n",
    "\n",
    "def import_data(path, zip=False):\n",
    "    if zip:\n",
    "        !unzip -q $path\n",
    "    training_data = np.load('training_data.npy')\n",
    "    valid_periods = np.load('valid_periods.npy')\n",
    "    categories = np.load('categories.npy')\n",
    "    return training_data, valid_periods, categories\n",
    "\n",
    "# Call the function to import the data\n",
    "training_data, valid_periods, categories = import_data('training_dataset.zip', zip=False)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(training_data.shape)\n",
    "print(valid_periods.shape)\n",
    "print(categories.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------* DATA ANALYSIS *---------------------------------------\n",
    "\n",
    "# Note that all the values are store as float64, so we can convert them to float32 to save memory\n",
    "if training_data.dtype == np.float64:\n",
    "    training_data = training_data.astype(np.float32)\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-< FIRST PLOTS AND PADDING REMOVAL >->->->->->->->->->->-\n",
    "\n",
    "# Plot the first 10 time series\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].plot(training_data[i])\n",
    "    axs[i].set_title('Category: {}'.format(categories[i]), fontsize=10)\n",
    "    axs[i].set_xlabel('Time', fontsize=10)\n",
    "    axs[i].set_ylabel('Value', fontsize=10)\n",
    "\n",
    "fig.suptitle('First 10 time series', fontsize=30)\n",
    "\n",
    "# Define a function to remove the padding from the time series\n",
    "\n",
    "def remove_padding(data, valid_periods):\n",
    "    data_no_pad = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_no_pad.append(data[i, valid_periods[i, 0]:valid_periods[i, 1]])\n",
    "    return np.array(data_no_pad)\n",
    "\n",
    "# Remove the padding from the time series\n",
    "training_data_no_pad = remove_padding(training_data, valid_periods)\n",
    "\n",
    "# Plot the first 10 time series without padding, keeping the information about the orignal temporal location \n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].plot(range(valid_periods[i, 0], valid_periods[i, 1]), training_data_no_pad[i])\n",
    "    axs[i].set_title('Category: {}'.format(categories[i]), fontsize=10)\n",
    "    axs[i].set_xlabel('Time', fontsize=10)\n",
    "    axs[i].set_ylabel('Value', fontsize=10)\n",
    "\n",
    "fig.suptitle('First 10 time series without padding', fontsize=30)\n",
    "\n",
    "# All this samples belong to category D, and are \"to be understood as not closely related to each other, but only as collected from similar data sources\"\n",
    "# This means that the time seies within the same categoty are to be considered as uncorrelated, but should there not be a correlationbetween the catoegories?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -<-<-<-<-<-<-<-<-<-< DATA DISTRIBUTION ANALYSIS >->->->->->->->->->->-\n",
    "\n",
    "\n",
    "# GROUP DATA BY CATEGORY:\n",
    "\n",
    "# Transform the categories into numbers\n",
    "categorical_to_numerical = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F':5}\n",
    "\n",
    "if type(categories[0]) == str:\n",
    "    for category in np.unique(categories):\n",
    "        categories[categories == category] = categorical_to_numerical[category]\n",
    "    # To save memory, convert the categories to int32\n",
    "    categories = categories.astype(np.int32)\n",
    "\n",
    "# Now plot the distribution of the time series over the different categories\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.hist(categories, bins=6)\n",
    "ax.set_title('Distribution of the time series over the different categories', fontsize=20)\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Number of time series')\n",
    "# separate the bars and use different colors\n",
    "for i in range(6):\n",
    "    ax.patches[i].set_color('C{}'.format(i))\n",
    "plt.xticks(np.arange(6))\n",
    "\n",
    "# OSS: a visible imbalance between the categories is present, with category 5 been extremely underrepresented \n",
    "# category 1 is just slightly underrepresented, while the other categories are more or less equally represented)\n",
    "\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-<-<-< DATA NORMLIZATION >->->->->->->->->->->->->-\n",
    "# AS THE DATA HAS ALREADY BEEN NORMALIZED (EACH ROW HAS BEEN NORMALIZED INDIPENDENTLY)\n",
    "# WE DEEMED THIS STEP NOT NECESSARY\n",
    "\n",
    "\n",
    "# Define min_max normalization function\n",
    "def min_max_norm(data, min, max):\n",
    "    return (data - min) / (max - min)\n",
    "\n",
    "# Apply the function over the categories singularly\n",
    "def min_max_norm_by_category(data, categories):\n",
    "    # Loop over the categories\n",
    "    for category in np.unique(categories):\n",
    "        # For each category compute the min and max\n",
    "        C_min = np.min(data[categories == category])\n",
    "        C_max = np.max(data[categories == category])\n",
    "        # Apply the min_max_norm function to the data of the current category\n",
    "        data[categories == category] = min_max_norm(data[categories == category], C_min, C_max)\n",
    "    return data\n",
    "\n",
    "\n",
    "# -<-<-<-<-<-<-< PLOT OF FIRST TIME SERIES FOR DIFFERENT CATEGORIES >->->->->->-\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(6):\n",
    "    axs[i].plot(training_data_no_pad[categories == i][0])\n",
    "    axs[i].set_title('Category: {}'.format(i), fontsize=10)\n",
    "    axs[i].set_xlabel('Time', fontsize=10)\n",
    "    axs[i].set_ylabel('Value', fontsize=10)\n",
    "\n",
    "fig.suptitle('First time series for each category', fontsize=30)\n",
    "\n",
    "# -<-<-<-<-<-<-< PLOT OF TIME SERIES BY LENGHT AND DOMAIN  >->->->->->-\n",
    "\n",
    "# Compute mean lenght of the time series for each category and plot them in a bar plot\n",
    "mean_lenght = []\n",
    "for i in range(6):\n",
    "    mean_lenght.append(np.mean(valid_periods[categories == i, 1] - valid_periods[categories == i, 0]))\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.bar(np.arange(6), mean_lenght)\n",
    "ax.set_title('Mean lenght of the time series for each category', fontsize=20)\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Mean lenght')\n",
    "plt.xticks(np.arange(6))\n",
    "for i in range(6):\n",
    "    ax.patches[i].set_color('C{}'.format(i))\n",
    "\n",
    "# OSS: a fairly similar mean lenght for all the categories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some Hyperparameters for the forcasting model\n",
    "WINDOW_SIZE = 200\n",
    "BATCH_SIZE = 32\n",
    "STRIDE = 10\n",
    "EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TELESCOPE = 18\n",
    "\n",
    "# In this case, given the nature of the problem, as well as the fact that the time series are not correlated\n",
    "# (so introducing some of them in training phase should now bias the test), we decide\n",
    "# to make use of the great variaty of time series available, and split data among the time series themselves:\n",
    "\n",
    "# This way, a certain percentage of the time series will be used for training (80% initial partition), while the remaining ones will be used for validation (20).\n",
    "# In both case, we define the number of samples to be predicted as TELESCOPE, and the number of samples to be used for the prediction as WINDOW_SIZE.\n",
    "\n",
    "# For the testing, we respect the imbalances among the different domains using stratify based on the categories\n",
    "\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(training_data_no_pad, categories, test_size=VALIDATION_SPLIT, stratify=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------* DATA PREPROCESSING *---------------------------------------\n",
    "# Build the sequences for the forecasting model: in this case, we will try to take into account the \n",
    "# imbalance between the categories usign a weighted loss function\n",
    "\n",
    "# The function lets us build the sequences to use for the training of the forecasting model:\n",
    "# In this case, given a certain domain (category), the function will build the sequences to be used for the training\n",
    "# by extracting temporal windows out of each time series.\n",
    "# Please note tha some of the time series will takes as they are, since they are shorter than the window size.\n",
    "# In some other cases, the time series are \"partioned\" in windows of the same size, and the last window is padded with zeros\n",
    "# In any case, for the correct creation of the sequences, dataset to be passed to the function must be the one without padding\n",
    "def build_sequences_on_domain(data = training_data_no_pad, domain = 0, window=200, stride=20, telescope=100):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = data[categories == domain]\n",
    "    temp_label = temp_df\n",
    "    # For each time series of the specific domain, compute the padding lenght\n",
    "    # OSS... This is done on the data without padding\n",
    "    for time_series_n in len(temp_df):\n",
    "        padding_len = window - len(temp_df[time_series_n])%window\n",
    "        if(padding_len != 0):\n",
    "            padding = np.zeros((padding_len), dtype='float32')\n",
    "            temp_df[time_series_n] = np.concatenate((padding,temp_df[time_series_n]))\n",
    "            padding = np.zeros((padding_len), dtype='float32')\n",
    "            temp_label[time_series_n] = np.concatenate((padding,temp_label[time_series_n]))\n",
    "        assert len(temp_df[time_series_n]) % window == 0\n",
    "\n",
    "    for time_series_n in range(len(temp_df)):\n",
    "        # Extract the temporal windows from the time series\n",
    "        if len(temp_df[time_series_n]) < window + telescope:\n",
    "            # Generate a padding of dimension telescope to be addded at the beginning of the time series\n",
    "            padding = np.zeros((telescope), dtype='float32')\n",
    "            temp_df[time_series_n] = np.concatenate((padding,temp_df[time_series_n]))\n",
    "            dataset.append(temp_df[time_series_n][0:window])\n",
    "            labels.append(temp_label[time_series_n][window:window+telescope])\n",
    "        else:\n",
    "            for i in np.arange(0, len(temp_df[time_series_n])-window-telescope, stride):\n",
    "                dataset.append(temp_df[time_series_n][i:i+window])\n",
    "                labels.append(temp_label[time_series_n][i+window:i+window+telescope])\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "    return dataset, labels\n",
    "\n",
    "def inspect_multivariate(X, y, columns, telescope, idx=None):\n",
    "    if(idx==None):\n",
    "        idx=np.random.randint(0,len(X))\n",
    "\n",
    "    # Plot three sequences chosen based on idx\n",
    "    figs, axs = plt.subplots(3, 1, sharex=True, figsize=(17,17))\n",
    "    for i in range(idx, idx+3):\n",
    "        axs[i].plot(np.arange(len(X[i])), X[i])\n",
    "        axs[i].scatter(np.arange(len(X[i]), len(X[i])+telescope), y[i], color='orange')\n",
    "        axs[i].set_title('Sequence {}'.format(i))\n",
    "        axs[i].set_ylim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "for i in range(6):\n",
    "    X_train_temp, y_train_temp = build_sequences_on_domain(data=X_train_raw, domain=i, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "    X_val_temp, y_val_temp = build_sequences_on_domain(data=X_val_raw, domain=i, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "    # Concatenate with the previous domains\n",
    "    X_train = np.concatenate((X_train, X_train_temp))\n",
    "    y_train = np.concatenate((y_train, y_train_temp))\n",
    "    X_val = np.concatenate((X_val, X_val_temp))\n",
    "    y_val = np.concatenate((y_val, y_val_temp))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
