{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------* FIRST ATTEMPT *---------------------------------------\n",
    "# Problem description: develop a forecasting model that is able to predict several uncorrelated time series\n",
    "\n",
    "# DATA STRUCTURE: \n",
    "# Single folder containing the following files:\n",
    "# -> 'training_data.npy': it contains a numpy array of shape (48000, 2776). 48000 time series of length 2776.\n",
    "# -> 'valid_periods.npy': it contains a numpy array of type (48000, 2) containing for each of the time series the start and end index of the current series, i.e. the part without padding.\n",
    "# -> 'categories.npy': it contains a numpy array of shape (48000,), containing for each of the time series the code of its category. The possible categories are in {'A', 'B', 'C', 'D', 'E', 'F'}.\n",
    "# IMPORTANT: This is a dataset consisting of monovariate time series, i.e. composed of a single feature, belonging to six different domains. The time series of each domain are not to be understood as closely related to each other, but only as collected from similar data sources.\n",
    "# What is required of you is therefore to build a model that is capable of generalising sufficiently to predict the future samples of the 60 time series of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and connect to drive personal folder\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/homework_2/Edoardo\n",
    "\n",
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Import other support libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for data inport:\n",
    "# The function takes as input the path of the folder containing the data and returns the training data, the validation periods and the categories\n",
    "# Before doing so, it eventually unzips it if the zip flag is set to True\n",
    "\n",
    "def import_data(path, zip=False):\n",
    "    if zip:\n",
    "        !unzip -q $path\n",
    "    training_data = np.load('training_data.npy')\n",
    "    valid_periods = np.load('valid_periods.npy')\n",
    "    categories = np.load('categories.npy')\n",
    "    return training_data, valid_periods, categories\n",
    "\n",
    "# Call the function to import the data\n",
    "training_data, valid_periods, categories = import_data('training_dataset.zip', zip=False)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(training_data.shape)\n",
    "print(valid_periods.shape)\n",
    "print(categories.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------* DATA ANALYSIS *---------------------------------------\n",
    "\n",
    "# Note that all the values are store as float64, so we can convert them to float32 to save memory\n",
    "if training_data.dtype == np.float64:\n",
    "    training_data = training_data.astype(np.float32)\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-< FIRST PLOTS AND PADDING REMOVAL >->->->->->->->->->->-\n",
    "\n",
    "# Plot the first 10 time series\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].plot(training_data[i])\n",
    "    axs[i].set_title('Category: {}'.format(categories[i]), fontsize=10)\n",
    "    axs[i].set_xlabel('Time', fontsize=10)\n",
    "    axs[i].set_ylabel('Value', fontsize=10)\n",
    "\n",
    "fig.suptitle('First 10 time series', fontsize=30)\n",
    "\n",
    "# Define a function to remove the padding from the time series\n",
    "\n",
    "def remove_padding(data, valid_periods):\n",
    "    data_no_pad = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_no_pad.append(data[i, valid_periods[i, 0]:valid_periods[i, 1]])\n",
    "    return np.array(data_no_pad)\n",
    "\n",
    "# Remove the padding from the time series\n",
    "training_data_no_pad = remove_padding(training_data, valid_periods)\n",
    "\n",
    "# Plot the first 10 time series without padding, keeping the information about the orignal temporal location \n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].plot(range(valid_periods[i, 0], valid_periods[i, 1]), training_data_no_pad[i])\n",
    "    axs[i].set_title('Category: {}'.format(categories[i]), fontsize=10)\n",
    "    axs[i].set_xlabel('Time', fontsize=10)\n",
    "    axs[i].set_ylabel('Value', fontsize=10)\n",
    "\n",
    "fig.suptitle('First 10 time series without padding', fontsize=30)\n",
    "\n",
    "# All this samples belong to category D, and are \"to be understood as not closely related to each other, but only as collected from similar data sources\"\n",
    "# This means that the time seies within the same categoty are to be considered as uncorrelated, but should there not be a correlationbetween the catoegories?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -<-<-<-<-<-<-<-<-<-< DATA DISTRIBUTION ANALYSIS >->->->->->->->->->->-\n",
    "\n",
    "\n",
    "# GROUP DATA BY CATEGORY:\n",
    "\n",
    "# Transform the categories into numbers\n",
    "categorical_to_numerical = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F':5}\n",
    "\n",
    "if categories[0] == \"D\":\n",
    "    for category in np.unique(categories):\n",
    "        categories[categories == category] = categorical_to_numerical[category]\n",
    "    # To save memory, convert the categories to int32\n",
    "    categories = categories.astype(np.int32)\n",
    "\n",
    "# Now plot the distribution of the time series over the different categories\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.hist(categories, bins=6)\n",
    "ax.set_title('Distribution of the time series over the different categories', fontsize=20)\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Number of time series')\n",
    "# separate the bars and use different colors\n",
    "for i in range(6):\n",
    "    ax.patches[i].set_color('C{}'.format(i))\n",
    "plt.xticks(np.arange(6))\n",
    "\n",
    "# OSS: a visible imbalance between the categories is present, with category 5 been extremely underrepresented \n",
    "# (category 1 is just slightly underrepresented, while the other categories are more or less equally represented)\n",
    "\n",
    "# Compute class_weight paramenter to be used for the fit of the model\n",
    "weights = {}\n",
    "for i in range(6):\n",
    "    weights[i] = 1 / np.sum(categories == i)\n",
    "print(weights)\n",
    "\n",
    "\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-<-<-< DATA NORMLIZATION >->->->->->->->->->->->->-\n",
    "# AS THE DATA HAS ALREADY BEEN NORMALIZED (EACH ROW HAS BEEN NORMALIZED INDIPENDENTLY)\n",
    "# WE DEEMED THIS STEP NOT NECESSARY\n",
    "\n",
    "\n",
    "# Define min_max normalization function\n",
    "def min_max_norm(data, min, max):\n",
    "    return (data - min) / (max - min)\n",
    "\n",
    "# Apply the function over the categories singularly\n",
    "def min_max_norm_by_category(data, categories):\n",
    "    # Loop over the categories\n",
    "    for category in np.unique(categories):\n",
    "        # For each category compute the min and max\n",
    "        C_min = np.min(data[categories == category])\n",
    "        C_max = np.max(data[categories == category])\n",
    "        # Apply the min_max_norm function to the data of the current category\n",
    "        data[categories == category] = min_max_norm(data[categories == category], C_min, C_max)\n",
    "    return data\n",
    "\n",
    "\n",
    "# -<-<-<-<-<-<-< PLOT OF FIRST TIME SERIES FOR DIFFERENT CATEGORIES >->->->->->-\n",
    "# Plot the first time series for each category\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(6):\n",
    "    axs[i].plot(training_data_no_pad[categories == i][0])\n",
    "    axs[i].set_title('Category: {}'.format(i), fontsize=10)\n",
    "    axs[i].set_xlabel('Time', fontsize=10)\n",
    "    axs[i].set_ylabel('Value', fontsize=10)\n",
    "\n",
    "# -<-<-<-<-<-<-< PLOT OF TIME SERIES BY LENGHT AND DOMAIN  >->->->->->-\n",
    "\n",
    "# Compute mean lenght of the time series for each category and plot them in a bar plot\n",
    "mean_lenght = []\n",
    "for i in range(6):\n",
    "    mean_lenght.append(np.mean(valid_periods[categories == i, 1] - valid_periods[categories == i, 0]))\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.bar(np.arange(6), mean_lenght)\n",
    "ax.set_title('Mean lenght of the time series for each category', fontsize=20)\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Mean lenght')\n",
    "plt.xticks(np.arange(6))\n",
    "for i in range(6):\n",
    "    ax.patches[i].set_color('C{}'.format(i))\n",
    "\n",
    "# OSS: a fairly similar mean lenght for all the categories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some Hyperparameters for the forcasting model\n",
    "WINDOW_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "STRIDE = 10\n",
    "EPOCHS = 200\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.01\n",
    "TELESCOPE = 18\n",
    "\n",
    "# In this case, given the nature of the problem, as well as the fact that the time series are not correlated\n",
    "# (so introducing some of them in training phase should now bias the test), we decide\n",
    "# to make use of the great variaty of time series available, and split data among the time series themselves:\n",
    "\n",
    "# This way, a certain percentage of the time series will be used for training (80% initial partition), while the remaining ones will be used for validation (20).\n",
    "# In both case, we define the number of samples to be predicted as TELESCOPE, and the number of samples to be used for the prediction as WINDOW_SIZE.\n",
    "\n",
    "# For the testing, we respect the imbalances among the different domains using stratify based on the categories\n",
    "\n",
    "X_train_raw, X_test_raw, domains_train, domains_test = train_test_split(training_data_no_pad, categories, test_size=TEST_SPLIT, stratify=categories)\n",
    "X_train_raw, X_val_raw, domains_train, domains_val = train_test_split(X_train_raw, domains_train, test_size=VALIDATION_SPLIT, stratify=domains_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------* DATA PREPROCESSING *---------------------------------------\n",
    "# Build the sequences for the forecasting model: in this case, we will try to take into account the \n",
    "# imbalance between the categories usign a weighted loss function\n",
    "\n",
    "# The function lets us build the sequences to use for the training of the forecasting model:\n",
    "# In this case, given a certain domain (category), the function will build the sequences to be used for the training\n",
    "# by extracting temporal windows out of each time series.\n",
    "# Please note tha some of the time series will takes as they are, since they are shorter than the window size.\n",
    "# In some other cases, the time series are \"partioned\" in windows of the same size, and the last window is padded with zeros\n",
    "# In any case, for the correct creation of the sequences, dataset to be passed to the function must be the one without padding\n",
    "def build_sequences_on_domain(data, categories_split  ,domain = 0, window=200, stride=20, telescope=100):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = data[categories_split == domain]\n",
    "    temp_label = temp_df.copy()\n",
    "    # For each time series of the specific domain, compute the padding lenght\n",
    "    # OSS... This is done on the data without padding\n",
    "    for time_series_n in range(len(temp_df)):\n",
    "        padding_len = (window + telescope) - len(temp_df[time_series_n])\n",
    "        if(padding_len > 0):\n",
    "            padding = np.zeros((padding_len,), dtype='float32')\n",
    "            temp_df[time_series_n] = np.concatenate((padding,temp_df[time_series_n]))\n",
    "            padding = np.zeros((padding_len,), dtype='float32')\n",
    "            temp_label[time_series_n] = np.concatenate((padding,temp_label[time_series_n]))\n",
    "        \n",
    "\n",
    "    for time_series_n in range(len(temp_df)):\n",
    "          for i in np.arange(0, len(temp_df[time_series_n])-window-telescope, stride):\n",
    "              dataset.append(temp_df[time_series_n][i:i+window])\n",
    "              labels.append(temp_label[time_series_n][i+window:i+window+telescope])\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "    return dataset, labels\n",
    "\n",
    "def inspect_multivariate(X, y, telescope, idx=None, data_to_plot= 10):\n",
    "    if(idx==None):\n",
    "        idx=np.random.randint(0,len(X))\n",
    "\n",
    "    # Plot three sequences chosen based on idx\n",
    "    figs, axs = plt.subplots(data_to_plot, 1, sharex=True, figsize=(30,15))\n",
    "    for i in range(idx, idx+data_to_plot):\n",
    "        axs[i-idx].plot(np.arange(len(X[i])), X[i])\n",
    "        axs[i-idx].scatter(np.arange(len(X[i]), len(X[i])+telescope), y[i], color='orange')\n",
    "        axs[i-idx].set_title('Sequence {}'.format(i))\n",
    "        axs[i-idx].set_ylim(0,1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def inspect_multivariate_prediction(X, y, pred, telescope, idx=None, data_to_plot= 10):\n",
    "    if(idx==None):\n",
    "        idx=np.random.randint(0,len(X))\n",
    "\n",
    "    # Plot data_to_plot sequences chosen based on idx\n",
    "    figs, axs = plt.subplots(data_to_plot, 1, sharex=True, figsize=(30,15))\n",
    "    for i in range(idx, idx+data_to_plot):\n",
    "        axs[i-idx].plot(np.arange(len(X[i])), X[i])\n",
    "        axs[i-idx].scatter(np.arange(len(X[i]), len(X[i])+telescope), y[i], color='orange')\n",
    "        axs[i-idx].scatter(np.arange(len(X[i]), len(X[i])+telescope), pred[i], color='green')\n",
    "        axs[i-idx].set_title('Sequence {}'.format(i))\n",
    "        axs[i-idx].set_ylim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = build_sequences_on_domain(data=X_train_raw,categories_split=domains_train, domain = 0, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "X_val, y_val = build_sequences_on_domain(data=X_val_raw,categories_split = domains_val,domain=0, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "X_test, y_test = build_sequences_on_domain(data=X_test_raw,categories_split = domains_test,domain=0, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "for i in range(1,6):\n",
    "    X_train_temp, y_train_temp = build_sequences_on_domain(data=X_train_raw,categories_split=domains_train, domain=i, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "    X_val_temp, y_val_temp = build_sequences_on_domain(data=X_val_raw,categories_split = domains_val,domain=i, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "    X_test_temp, y_test_temp = build_sequences_on_domain(data=X_test_raw,categories_split = domains_test,domain=i, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "    # Concatenate with the previous domains\n",
    "    X_train = np.concatenate((X_train, X_train_temp))\n",
    "    y_train = np.concatenate((y_train, y_train_temp))\n",
    "    X_val = np.concatenate((X_val, X_val_temp))\n",
    "    y_val = np.concatenate((y_val, y_val_temp))\n",
    "    X_test = np.concatenate((X_test, X_test_temp))\n",
    "    y_test = np.concatenate((y_test, y_test_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_multivariate(X_train, y_train, TELESCOPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output shape for the forecasting model\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "output_shape = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------* FORECASTING MODEL DEFINITION     *---------------------------------------\n",
    "\n",
    "def build_CONV_LSTM_model(input_shape, output_shape):\n",
    "    # Ensure the input time steps are at least as many as the output time steps\n",
    "    assert input_shape[0] >= output_shape[0], \"For this exercise we want input time steps to be >= of output time steps\"\n",
    "\n",
    "    # Define the input layer with the specified shape\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Add a Bidirectional LSTM layer with 64 units\n",
    "    x = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True, name='lstm'), name='bidirectional_lstm')(input_layer)\n",
    "\n",
    "    # Add a 1D Convolution layer with 128 filters and a kernel size of 3\n",
    "    x = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='conv')(x)\n",
    "\n",
    "    # Add a final Convolution layer to match the desired output shape\n",
    "    output_layer = tfkl.Conv1D(1, 3, padding='same', name='output_layer')(x)\n",
    "\n",
    "    # Calculate the size to crop from the output to match the output shape\n",
    "    crop_size = output_layer.shape[1] - output_shape[0]\n",
    "\n",
    "    # Crop the output to the desired length\n",
    "    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)\n",
    "\n",
    "    # Construct the model by connecting input and output layers\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n",
    "\n",
    "    # Compile the model with Mean Squared Error loss and Adam optimizer\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------------* LR SCHEDULER  *---------------------------------------\n",
    "\n",
    "\n",
    "def lr_warmup_cosine_decay(global_step,\n",
    "                           warmup_steps,\n",
    "                           hold = 0,\n",
    "                           total_steps=0,\n",
    "                           start_lr=0.0,\n",
    "                           target_lr=1e-3):\n",
    "    # Cosine decay\n",
    "    learning_rate = 0.5 * target_lr * (1 + np.cos(np.pi * (global_step - warmup_steps - hold) / float(total_steps - warmup_steps - hold)))\n",
    "\n",
    "    # Target LR * progress of warmup (=1 at the final warmup step)\n",
    "    warmup_lr = target_lr * (global_step / warmup_steps)\n",
    "\n",
    "    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.\n",
    "    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n",
    "    if hold > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold,\n",
    "                                 learning_rate, target_lr)\n",
    "\n",
    "    learning_rate = np.where(global_step < warmup_steps, warmup_lr, learning_rate)\n",
    "    return learning_rate\n",
    "\n",
    "# Define callback for learning rate scheduler and warmup\n",
    "\n",
    "class WarmupCosineDecay(tfk.callbacks.Callback):\n",
    "    def __init__(self, total_steps=0, warmup_steps=0, start_lr=0.0, target_lr=1e-3, hold=0):\n",
    "\n",
    "        super(WarmupCosineDecay, self).__init__()\n",
    "        self.start_lr = start_lr\n",
    "        self.hold = hold\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = 0\n",
    "        self.target_lr = target_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.lrs = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = model.optimizer.lr.numpy()\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = lr_warmup_cosine_decay(global_step=self.global_step,\n",
    "                                    total_steps=self.total_steps,\n",
    "                                    warmup_steps=self.warmup_steps,\n",
    "                                    start_lr=self.start_lr,\n",
    "                                    target_lr=self.target_lr,\n",
    "                                    hold=self.hold)\n",
    "        tfk.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "\n",
    "\n",
    "total_steps = len(X_train)/BATCH_SIZE *EPOCHS\n",
    "\n",
    "warmup_steps = int(0.05*total_steps)\n",
    "\n",
    "\n",
    "# Print learning rate on tensorboard: write appropriate callback\n",
    "\n",
    "class LRTensorBoard(TensorBoard):\n",
    "    # add other arguments to __init__ if you need\n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super().__init__(log_dir=log_dir, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs.update({'lr': K.eval(self.model.optimizer.lr)})\n",
    "        super().on_epoch_end(epoch, logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_CONV_LSTM_model(input_shape, output_shape)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split=(X_val, y_val),\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=12, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5)\n",
    "    ],\n",
    "    class_weight=weights\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set using the model\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = tfk.metrics.mean_squared_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test.flatten(), predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_multivariate_prediction(X_test, y_test, predictions, TELESCOPE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
