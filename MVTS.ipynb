{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------* ATTENTION ATTEMPT *---------------------------------------\n",
    "# Implementation of a model inspired by the paper \"A Transformer-based Framework for Multivariate Time Series Representation Learning\", \n",
    "# link: https://dl.acm.org/doi/10.1145/3447548.3467401\n",
    "\n",
    "# Problem description: develop a forecasting model that is able to predict several uncorrelated time series\n",
    "\n",
    "# DATA STRUCTURE: \n",
    "# Single folder containing the following files:\n",
    "# -> 'training_data.npy': it contains a numpy array of shape (48000, 2776). 48000 time series of length 2776.\n",
    "# -> 'valid_periods.npy': it contains a numpy array of type (48000, 2) containing for each of the time series the start and end index of the current series, i.e. the part without padding.\n",
    "# -> 'categories.npy': it contains a numpy array of shape (48000,), containing for each of the time series the code of its category. The possible categories are in {'A', 'B', 'C', 'D', 'E', 'F'}.\n",
    "# IMPORTANT: This is a dataset consisting of monovariate time series, i.e. composed of a single feature, belonging to six different domains. The time series of each domain are not to be understood as closely related to each other, but only as collected from similar data sources.\n",
    "# What is required of you is therefore to build a model that is capable of generalising sufficiently to predict the future samples of the 60 time series of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and connect to drive personal folder\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/homework_2/Edoardo\n",
    "\n",
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Import other support libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for data inport:\n",
    "# The function takes as input the path of the folder containing the data and returns the training data, the validation periods and the categories\n",
    "# Before doing so, it eventually unzips it if the zip flag is set to True\n",
    "\n",
    "def import_data(path, zip=False):\n",
    "    if zip:\n",
    "        !unzip -q $path\n",
    "    training_data = np.load('training_data.npy')\n",
    "    valid_periods = np.load('valid_periods.npy')\n",
    "    categories = np.load('categories.npy')\n",
    "    return training_data, valid_periods, categories\n",
    "\n",
    "# Call the function to import the data\n",
    "training_data, valid_periods, categories = import_data('training_dataset.zip', zip=False)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(training_data.shape)\n",
    "print(valid_periods.shape)\n",
    "print(categories.shape)\n",
    "\n",
    "# Transform the data by extending the dimensions (number of time series, length of time series, number of features)\n",
    "training_data_no_pad = np.expand_dims(training_data, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------* DATA ANALYSIS *---------------------------------------\n",
    "\n",
    "# Note that all the values are store as float64, so we can convert them to float32 to save memory\n",
    "if training_data.dtype == np.float64:\n",
    "    training_data = training_data.astype(np.float32)\n",
    "\n",
    "# Then transform the categories into numbers\n",
    "categorical_to_numerical = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F':5}\n",
    "\n",
    "if categories[0] == \"D\":\n",
    "    for category in np.unique(categories):\n",
    "        categories[categories == category] = categorical_to_numerical[category]\n",
    "    # To save memory, convert the categories to int32\n",
    "    categories = categories.astype(np.int32)\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-< FIRST PLOTS AND PADDING REMOVAL/NORMALIZATION >->->->->->->->->->->-\n",
    "\n",
    "# Plot the first 10 time series\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].plot(training_data[i])\n",
    "    axs[i].set_title('Category: {}'.format(categories[i]), fontsize=10)\n",
    "    axs[i].set_xlabel('Time', fontsize=10)\n",
    "    axs[i].set_ylabel('Value', fontsize=10)\n",
    "\n",
    "fig.suptitle('First 10 time series', fontsize=30)\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-<-<-< DATA NORMLIZATION >->->->->->->->->->->->->-\n",
    "# AS THE DATA HAS ALREADY BEEN NORMALIZED (EACH ROW HAS BEEN NORMALIZED INDIPENDENTLY)\n",
    "# WE DEEMED THIS STEP NOT NECESSARY\n",
    "\n",
    "# Define min_max normalization function\n",
    "def min_max_norm(data, min, max):\n",
    "    return (data - min) / (max - min)\n",
    "\n",
    "# Apply the function over the categories singularly\n",
    "def min_max_norm_by_category(data, categories):\n",
    "    # Loop over the categories\n",
    "    for category in np.unique(categories):\n",
    "        # For each category compute the min and max\n",
    "        C_min = np.min(data[categories == category])\n",
    "        C_max = np.max(data[categories == category])\n",
    "        # Apply the min_max_norm function to the data of the current category\n",
    "        data[categories == category] = min_max_norm(data[categories == category], C_min, C_max)\n",
    "    return data\n",
    "\n",
    "# Function to apply a robust scaler on each non-padded time series\n",
    "# The function takes as input the data and loops over each time series, applying the robust scaler to each of them\n",
    "def robust_scaler_normalization(data):\n",
    "    # Loop over the time series\n",
    "    for i in range(data.shape[0]):\n",
    "        # Apply the robust scaler to the current time series\n",
    "        data[i, :, 0] = RobustScaler().fit_transform(data[i, :, 0])\n",
    "    return data\n",
    "\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-<  PADDING REMOVAL >->->->->->->->->->-\n",
    "# Define a function to remove the padding from the time serie\n",
    "\n",
    "def remove_padding(data, valid_periods):\n",
    "    data_no_pad = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_no_pad.append(data[i, valid_periods[i, 0]:valid_periods[i, 1]])\n",
    "    return np.array(data_no_pad)\n",
    "\n",
    "\n",
    "# Remove the padding from the time series\n",
    "training_data_no_pad = remove_padding(training_data_no_pad, valid_periods)\n",
    "\n",
    "# Call the function for robust scaler normalization\n",
    "training_data_no_pad = robust_scaler_normalization(training_data_no_pad, categories)\n",
    "\n",
    "# Plot the first 10 time series without padding, keeping the information about the orignal temporal location\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].plot(range(valid_periods[i, 0], valid_periods[i, 1]), training_data_no_pad[i])\n",
    "    axs[i].set_title('Category: {}'.format(categories[i]), fontsize=10)\n",
    "    axs[i].set_xlabel('Time', fontsize=10)\n",
    "    axs[i].set_ylabel('Value', fontsize=10)\n",
    "\n",
    "fig.suptitle('First 10 time series without padding and normalized', fontsize=30)\n",
    "\n",
    "# All this samples belong to category D, and are \"to be understood as not closely related to each other, but only as collected from similar data sources\"\n",
    "# This means that the time seies within the same categoty are to be considered as uncorrelated, but should there not be a correlation between the catoegories?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -<-<-<-<-<-<-<-<-<-< DATA DISTRIBUTION ANALYSIS >->->->->->->->->->->-\n",
    "\n",
    "\n",
    "# GROUP DATA BY CATEGORY:\n",
    "\n",
    "# Transform the categories into numbers\n",
    "categorical_to_numerical = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F':5}\n",
    "\n",
    "if categories[0] == \"D\":\n",
    "    for category in np.unique(categories):\n",
    "        categories[categories == category] = categorical_to_numerical[category]\n",
    "    # To save memory, convert the categories to int32\n",
    "    categories = categories.astype(np.int32)\n",
    "\n",
    "# Now plot the distribution of the time series over the different categories\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.hist(categories, bins=6)\n",
    "ax.set_title('Distribution of the time series over the different categories', fontsize=20)\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Number of time series')\n",
    "# separate the bars and use different colors\n",
    "for i in range(6):\n",
    "    ax.patches[i].set_color('C{}'.format(i))\n",
    "plt.xticks(np.arange(6))\n",
    "\n",
    "# OSS: a visible imbalance between the categories is present, with category 5 been extremely underrepresented \n",
    "# (category 1 is just slightly underrepresented, while the other categories are more or less equally represented)\n",
    "\n",
    "# Compute class_weight paramenter to be used for the fit of the model\n",
    "weights = {}\n",
    "for i in range(6):\n",
    "    weights[i] = 1 / np.sum(categories == i)\n",
    "print(weights)\n",
    "\n",
    "\n",
    "# -<-<-<-<-<-<-< PLOT OF the FIRST TIME SERIES FOR DIFFERENT CATEGORIES >->->->->->-\n",
    "# Plot the first n time series for each category (6 diffrerent plots, with multiple time series in each plot)\n",
    "\n",
    "n = 10\n",
    "fig, axs = plt.subplots(6, 1, figsize=(20, 30))\n",
    "for i in range(6):\n",
    "    axs[i].plot(training_data_no_pad[categories == i][:n].T)\n",
    "    axs[i].set_title('Category: {}'.format(i), fontsize=20)\n",
    "    axs[i].set_xlabel('Time', fontsize=20)\n",
    "    axs[i].set_ylabel('Value', fontsize=20)\n",
    "\n",
    "# -<-<-<-<-<-<-< PLOT OF TIME SERIES BY LENGHT AND DOMAIN  >->->->->->-\n",
    "\n",
    "# Compute mean lenght of the time series for each category and plot them in a bar plot\n",
    "mean_lenght = []\n",
    "for i in range(6):\n",
    "    mean_lenght.append(np.mean(valid_periods[categories == i, 1] - valid_periods[categories == i, 0]))\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.bar(np.arange(6), mean_lenght)\n",
    "ax.set_title('Mean lenght of the time series for each category', fontsize=20)\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Mean lenght')\n",
    "plt.xticks(np.arange(6))\n",
    "for i in range(6):\n",
    "    ax.patches[i].set_color('C{}'.format(i))\n",
    "\n",
    "# OSS: a fairly similar mean lenght for all the categories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some Hyperparameters for the forcasting model\n",
    "WINDOW_SIZE = 200\n",
    "BATCH_SIZE = 256\n",
    "STRIDE = 10\n",
    "EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.03\n",
    "TELESCOPE = 18\n",
    "AUTOREGRESSIVE_TELESCOPE = 3\n",
    "assert AUTOREGRESSIVE_TELESCOPE < TELESCOPE\n",
    "\n",
    "# In this case, given the nature of the problem, as well as the fact that the time series are not correlated\n",
    "# (so introducing some of them in training phase should now bias the test), we decide\n",
    "# to make use of the great variaty of time series available, and split data among the time series themselves:\n",
    "\n",
    "# This way, a certain percentage of the time series will be used for training (80% initial partition), while the remaining ones will be used for validation (20).\n",
    "# In both case, we define the number of samples to be predicted as TELESCOPE, and the number of samples to be used for the prediction as WINDOW_SIZE.\n",
    "\n",
    "# For the testing, we respect the imbalances among the different domains using stratify based on the categories\n",
    "\n",
    "X_train_raw, X_test_raw, domains_train, domains_test = train_test_split(training_data_no_pad, categories, test_size=TEST_SPLIT, stratify=categories)\n",
    "X_train_raw, X_val_raw, domains_train, domains_val = train_test_split(X_train_raw, domains_train, test_size=VALIDATION_SPLIT, stratify=domains_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------* DATA PREPROCESSING *---------------------------------------\n",
    "# Build the sequences for the forecasting model: in this case, we will try to take into account the \n",
    "# imbalance between the categories usign a weighted loss function\n",
    "\n",
    "# The function lets us build the sequences to use for the training of the forecasting model:\n",
    "# In this case, given a certain domain (category), the function will build the sequences to be used for the training\n",
    "# by extracting temporal windows out of each time series.\n",
    "# Please note tha some of the time series will takes as they are, since they are shorter than the window size.\n",
    "# In some other cases, the time series are \"partioned\" in windows of the same size, and the last window is padded with zeros\n",
    "# In any case, for the correct creation of the sequences, dataset to be passed to the function must be the one without padding\n",
    "def build_sequences_on_domain(data, categories_split  ,domain = 0, window=200, stride=20, telescope=100):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    padding_mask = []\n",
    "    temp_df = data[categories_split == domain]\n",
    "    temp_label = temp_df.copy()\n",
    "    # For each time series of the specific domain, compute the padding lenght\n",
    "    # At the same time, generate the padding mask that should have dimension (batch_size, time_series_lenght):\n",
    "    # Ideally we generate a mask for lenght = time_series_lenght + padding_lenght for each signal, then\n",
    "    # we generate the padding mask during the generation of sequences using the masks previously generated\n",
    "    # OSS... This is done on the data without padding\n",
    "    masks = []\n",
    "    for time_series_n in range(len(temp_df)):\n",
    "        padding_len = (window + telescope) - len(temp_df[time_series_n])\n",
    "        if(padding_len > 0):\n",
    "            padding = np.zeros((padding_len,1), dtype='float32')\n",
    "            temp_df[time_series_n] = np.concatenate((padding,temp_df[time_series_n]))\n",
    "            padding = np.zeros((padding_len,1), dtype='float32')\n",
    "            temp_label[time_series_n] = np.concatenate((padding,temp_label[time_series_n]))\n",
    "            # Generate the padding mask\n",
    "            masks[time_series_n] = np.concatenate((np.zeros((padding_len), dtype='int32'), np.ones((len(temp_df[time_series_n])-padding_len), dtype='int32')))\n",
    "            \n",
    "        \n",
    "        \n",
    "    # Generate the sequences and proper padding masks\n",
    "    for time_series_n in range(len(temp_df)):\n",
    "          for i in np.arange(0, len(temp_df[time_series_n])-window-telescope, stride):\n",
    "              dataset.append(temp_df[time_series_n][i:i+window])\n",
    "              padding_mask.append(masks[time_series_n][i:i+window])\n",
    "              labels.append(temp_label[time_series_n][i+window:i+window+telescope])\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "    return dataset, labels\n",
    "\n",
    "def inspect_multivariate(X, y, telescope, idx=None, data_to_plot= 5):\n",
    "    if(idx==None):\n",
    "        idx=np.random.randint(0,len(X))\n",
    "\n",
    "    # Plot three sequences chosen based on idx\n",
    "    figs, axs = plt.subplots(data_to_plot, 1, sharex=True, figsize=(30,15))\n",
    "    for i in range(idx, idx+data_to_plot):\n",
    "        axs[i-idx].plot(np.arange(len(X[i])), X[i])\n",
    "        axs[i-idx].scatter(np.arange(len(X[i]), len(X[i])+telescope), y[i], color='orange')\n",
    "        axs[i-idx].set_title('Sequence {}'.format(i))\n",
    "        axs[i-idx].set_ylim(0,1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def inspect_multivariate_prediction(X, y, pred, telescope, idx=None, data_to_plot= 5):\n",
    "    if(idx==None):\n",
    "        idx=np.random.randint(0,len(X))\n",
    "\n",
    "    # Plot data_to_plot sequences chosen based on idx\n",
    "    figs, axs = plt.subplots(data_to_plot, 1, sharex=True, figsize=(30,15))\n",
    "    for i in range(idx, idx+data_to_plot):\n",
    "        axs[i-idx].plot(np.arange(len(X[i])), X[i])\n",
    "        axs[i-idx].scatter(np.arange(len(X[i]), len(X[i])+telescope), y[i], color='orange')\n",
    "        axs[i-idx].scatter(np.arange(len(X[i]), len(X[i])+telescope), pred[i], color='green')\n",
    "        axs[i-idx].set_title('Sequence {}'.format(i))\n",
    "        axs[i-idx].set_ylim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = build_sequences_on_domain(data=X_train_raw,categories_split=domains_train, domain = 0, window=WINDOW_SIZE, stride=STRIDE, telescope=AUTOREGRESSIVE_TELESCOPE)\n",
    "X_val, y_val = build_sequences_on_domain(data=X_val_raw,categories_split = domains_val,domain=0, window=WINDOW_SIZE, stride=STRIDE, telescope=AUTOREGRESSIVE_TELESCOPE)\n",
    "X_test_reg, y_test_reg = build_sequences_on_domain(data=X_test_raw,categories_split = domains_test,domain=0, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "for i in range(1,6):\n",
    "    X_train_temp, y_train_temp = build_sequences_on_domain(data=X_train_raw,categories_split=domains_train, domain=i, window=WINDOW_SIZE, stride=STRIDE, telescope=AUTOREGRESSIVE_TELESCOPE)\n",
    "    X_val_temp, y_val_temp = build_sequences_on_domain(data=X_val_raw,categories_split = domains_val,domain=i, window=WINDOW_SIZE, stride=STRIDE, telescope=AUTOREGRESSIVE_TELESCOPE)\n",
    "    X_test_temp, y_test_temp = build_sequences_on_domain(data=X_test_raw,categories_split = domains_test,domain=i, window=WINDOW_SIZE, stride=STRIDE, telescope=TELESCOPE)\n",
    "    # Concatenate with the previous domains\n",
    "    X_train = np.concatenate((X_train, X_train_temp))\n",
    "    y_train = np.concatenate((y_train, y_train_temp))\n",
    "    X_val = np.concatenate((X_val, X_val_temp))\n",
    "    y_val = np.concatenate((y_val, y_val_temp))\n",
    "    X_test_reg = np.concatenate((X_test_reg, X_test_temp))\n",
    "    y_test_reg = np.concatenate((y_test_reg, y_test_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_multivariate(X_train, y_train, AUTOREGRESSIVE_TELESCOPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output shape for the forecasting model\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test_reg.shape)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "output_shape = y_train.shape[1:]\n",
    "\n",
    "print(input_shape)\n",
    "print(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------------* FORECASTING MODEL DEFINITION *---------------------------------------\n",
    "\n",
    "# Define the forecasting model\n",
    "# The model is composed of a first encoder, that, in our case (being a univariate prediction problem) is composed of a single Conv1D layer\n",
    "# with 1 input size and d output size, where d is the number of filters (and the dimension of the latent space of the input embedding).\n",
    "# Each kernel K_i (i = 1, ... , d) has a a dimension of (k,m), where 𝑘 is the width in number of time steps and m is the number of features(variables, so 1 in this case).\n",
    "# Before sending the embedded input into the transformer, we add a positional encoding to the input: for this purpose, we define two different possible encodings:\n",
    "# -> a fixed encoding, based on sinuosoidal functions\n",
    "# -> a learned encoding, that is learned during the training phase\n",
    "# As proposed in the paper, we generate a padding mask which adds a large negative value to the attention scores for the padded positions in the signals whose valid lenght\n",
    "# is smaller than the window size.\n",
    "# Finally, the transformer is a simple FFN with batch normalization and dropout.\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-<-<- POSITIONAL ENCODING >->->->->->->->->->->->->-\n",
    "\n",
    "class FixedPositionalEncoding(tfk.Layer):\n",
    "    # d_model : dimension of the latent space of the input embedding\n",
    "    # dropout_rate : dropout rate to be applied to the input\n",
    "    # max_len : maximum lenght of the time series\n",
    "    def __init__(self, d_model, dropout_rate = 0.1, max_len=200, scale_factor=1.0):\n",
    "        super(FixedPositionalEncoding, self).__init__()\n",
    "        self.dropout = dropout_rate\n",
    "        # Compute the positional encoding vector for the given max_len (maximul lenght of the time series)\n",
    "        # for the given d_model (dimension of the latent space of the input embedding).\n",
    "        # For the i-th position of the input, the positional encoding is computed as:\n",
    "        pe = tf.zeros((max_len, d_model), dtype=tf.float32)\n",
    "        position = tf.range(0, max_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = tf.sin(position * div_term)\n",
    "        pe[:, 1::2] = tf.cos(position * div_term)\n",
    "        pe = scale_factor * tf.expand_dims(pe, axis=0)\n",
    "        self.pe = tf.Variable(pe, trainable=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.pe[:tf.shape(x)[0], :]\n",
    "        x = tfkl.Dropout(rate=self.dropout)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LearnablePositionalEncoding(tfk.Layer):\n",
    "\n",
    "    # Define the position encoding as trainable parameters\n",
    "    def __init__(self, d_model,dropout_rate = 0.1, max_len=1024):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.dropout = dropout_rate\n",
    "        self.pe = tf.Variable(tf.random.uniform((max_len, 1, d_model), minval=-0.02, maxval=0.02))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.pe[:tf.shape(x)[0], :]\n",
    "        x = tfkl.Dropout(rate=self.dropout)(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-<-<- TRANSFORMER DEFINITION >->->->->->->->->->->->->-\n",
    "\n",
    "# Define the transformer encoder layer with batch normalization instead of layer normalization\n",
    "\n",
    "class TransformerLayer(tfkl.Layer):\n",
    "    # d_model : dimension of the latent space of the input embedding\n",
    "    # nhead : number of heads in the multihead attention mechanism\n",
    "    # dim_feedforward : dimension of the feedforward network\n",
    "    # dropout : dropout rate to be applied in the net\n",
    "    # activation : activation function to be applied in the net\n",
    "    def __init__(self, d_model, nhead = 4, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        # Define all the layers in the model\n",
    "        self.self_attn = tfkl.MultiHeadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.dropout1 = tfkl.Dropout(rate=dropout)\n",
    "        self.batch_norm1 = tfkl.BatchNormalization(epsilon=1e-5)\n",
    "        self.dense1 = tfkl.Dense(dim_feedforward, activation=activation)\n",
    "        self.dropout2 = tfkl.Dropout(rate=dropout)\n",
    "        self.dense2 = tfkl.Dense(d_model)\n",
    "        self.dropout3 = tfkl.Dropout(rate=dropout)\n",
    "        self.batch_norm2 = tfkl.BatchNormalization(epsilon=1e-5)\n",
    "\n",
    "    def call(self, embedded_output, attention_mask=None,):\n",
    "        # Connect the different initialized layers:\n",
    "        # embedded_output : input-encoder output to be fed to the transformer\n",
    "        # attention_mask : mask to be applied to the attention layer to mask the padded positions\n",
    "        src = embedded_output\n",
    "        # -----* MULTIHEAD ATTENTION *-----\n",
    "        # Multiheaded, self-attention mechanism\n",
    "        src2 = self.self_attn(src, src, attention_mask=attention_mask)\n",
    "        # Apply dropout\n",
    "        src2 = self.dropout1(src2)\n",
    "        # -----* RESIDUAL CONNECTION AND BATCH NORMALIZATION *-----\n",
    "        # Residual connection\n",
    "        src = src + src2\n",
    "        # Apply batch normalization\n",
    "        src = self.batch_norm1(src)\n",
    "        # -----* FEEDFORWARD NETWORK *-----\n",
    "        # Apply first dense layer\n",
    "        src2 = self.dense1(src)\n",
    "        # Apply dropout\n",
    "        src2 = self.dropout2(src2)\n",
    "        # Apply second dense \n",
    "        src2 = self.dense2(src2)\n",
    "        # Apply dropout\n",
    "        src2 = self.dropout3(src2)\n",
    "        # Residual connection\n",
    "        src = src + src2\n",
    "        # Apply batch normalization\n",
    "        src = self.batch_norm2(epsilon=1e-5)(src)\n",
    "        return src\n",
    "    \n",
    "# Generate a class to stack multiple transformer layers\n",
    "class TransformerStack(tfkl.Layer):\n",
    "    def __init__(self, d_model, nhead = 4, dim_feedforward=2048, dropout=0.1, activation=\"relu\", layers=1):\n",
    "        super(TransformerStack, self).__init__()\n",
    "        self.layers = [TransformerLayer(d_model, nhead, dim_feedforward, dropout, activation) for _ in range(layers)]\n",
    "    \n",
    "    def call(self, embedded_output, attention_mask=None):\n",
    "        # Apply the attentoion mask to the first layer\n",
    "        src = self.layers[0](embedded_output, attention_mask)\n",
    "        # Apply the remaining layers\n",
    "        for layer in self.layers[1:]:\n",
    "            src = layer(src)\n",
    "        return src\n",
    "\n",
    "# -<-<-<-<-<-<-<-<-<-<-<- FORECASTING MODEL DEFINITION >->->->->->->->->->->->->-\n",
    "# The following function defines the forecasting model:\n",
    "def build_MVTSForecasting_model(input_shape,output_shape, padding_mask,encoder = 'conv1d',fixed_pos_encoding=False,, layers=1):\n",
    "    # Ensure the input time steps are at least as many as the output time steps\n",
    "    assert input_shape[0] >= output_shape[0], \"We want input time steps to be >= of output time steps\"\n",
    "\n",
    "    # Define some hyperparameters\n",
    "    max_len = input_shape[0]\n",
    "    d_model = 64\n",
    "    nhead = 4\n",
    "    dim_feedforward = 2048\n",
    "    dropout_rate_encoding = 0.1\n",
    "    dropout_rate_transformer = 0.1\n",
    "\n",
    "    # Defore using the padding mask, we need to reshape it\n",
    "    mask = tf.cast(tf.expand_dims(padding_mask, axis=1), \"int32\")\n",
    "\n",
    "    # Define the input layer with the specified shape\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Define the encoder layer\n",
    "    # The encoder is composed of a single Conv1D layer with 1 input size and d output size, where d is the number of filters (and the dimension of the latent space of the input embedding).\n",
    "    # Each kernel K_i (i = 1, ... , d) has a a dimension of (k,m), where 𝑘 is the width in number of time steps and m is the number of features(variables, so 1 in this case).\n",
    "\n",
    "    if encoder == 'conv1d':\n",
    "        encoder = tfkl.Conv1D(filters=d_model, kernel_size=5, strides=1, padding='same', name='encoder')(input_layer)\n",
    "    elif encoder == 'dense':\n",
    "        encoder = tfkl.Dense(d_model, activation='relu', name='encoder')(input_layer)\n",
    "\n",
    "    # Define the positional encoding layer\n",
    "    if fixed_pos_encoding:\n",
    "        pos_encoder = FixedPositionalEncoding(d_model, dropout_rate=dropout_rate_encoding, max_len=max_len, scale_factor=1.0)(encoder)\n",
    "    else:\n",
    "        pos_encoder = LearnablePositionalEncoding(d_model, dropout_rate=dropout_rate_encoding, max_len=max_len)(encoder)\n",
    "\n",
    "    # Proceed with the transformer layers\n",
    "    transformer = TransformerStack(d_model, nhead, dim_feedforward, dropout_rate_transformer, layers=layers)(pos_encoder, mask)\n",
    "    \n",
    "    # Add non-linear activation function\n",
    "    end = tfkl.Activation('relu')(transformer)\n",
    "\n",
    "    # Define dropout layer\n",
    "    end = tfkl.Dropout(0.1)(end)\n",
    "\n",
    "    output = output * tf.expand_dims(mask, axis=-1)\n",
    "\n",
    "    #Finally, the output layer is a simple dense layer with AUTOREGRESSIVE_TELESCOPE output size\n",
    "    output_layer = tfkl.Dense(output_shape[0], name='output_layer')(end)\n",
    "\n",
    "    # Construct the model by connecting input and output \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n",
    "\n",
    "    # Compile the model with Mean Squared Error loss and Adam optimizer\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------* LR SCHEDULER  *---------------------------------------\n",
    "\n",
    "\n",
    "def lr_warmup_cosine_decay(global_step,\n",
    "                           warmup_steps,\n",
    "                           hold = 0,\n",
    "                           total_steps=0,\n",
    "                           start_lr=0.0,\n",
    "                           target_lr=1e-3):\n",
    "    # Cosine decay\n",
    "    learning_rate = 0.5 * target_lr * (1 + np.cos(np.pi * (global_step - warmup_steps - hold) / float(total_steps - warmup_steps - hold)))\n",
    "\n",
    "    # Target LR * progress of warmup (=1 at the final warmup step)\n",
    "    warmup_lr = target_lr * (global_step / warmup_steps)\n",
    "\n",
    "    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.\n",
    "    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n",
    "    if hold > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold,\n",
    "                                 learning_rate, target_lr)\n",
    "\n",
    "    learning_rate = np.where(global_step < warmup_steps, warmup_lr, learning_rate)\n",
    "    return learning_rate\n",
    "\n",
    "# Define callback for learning rate scheduler and warmup\n",
    "\n",
    "class WarmupCosineDecay(tfk.callbacks.Callback):\n",
    "    def __init__(self, total_steps=0, warmup_steps=0, start_lr=0.0, target_lr=1e-3, hold=0):\n",
    "\n",
    "        super(WarmupCosineDecay, self).__init__()\n",
    "        self.start_lr = start_lr\n",
    "        self.hold = hold\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = 0\n",
    "        self.target_lr = target_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.lrs = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = model.optimizer.lr.numpy()\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = lr_warmup_cosine_decay(global_step=self.global_step,\n",
    "                                    total_steps=self.total_steps,\n",
    "                                    warmup_steps=self.warmup_steps,\n",
    "                                    start_lr=self.start_lr,\n",
    "                                    target_lr=self.target_lr,\n",
    "                                    hold=self.hold)\n",
    "        tfk.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "\n",
    "\n",
    "total_steps = len(X_train)/BATCH_SIZE *EPOCHS\n",
    "\n",
    "warmup_steps = int(0.05*total_steps)\n",
    "\n",
    "\n",
    "# Print learning rate on tensorboard: write appropriate callback\n",
    "\n",
    "class LRTensorBoard(TensorBoard):\n",
    "    # add other arguments to __init__ if you need\n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super().__init__(log_dir=log_dir, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs.update({'lr': K.eval(self.model.optimizer.lr)})\n",
    "        super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_Att_biLSTM_model(input_shape, output_shape)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=12, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5)\n",
    "    ],\n",
    "    class_weight=weights\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history['val_loss'])\n",
    "plt.figure(figsize=(17,4))\n",
    "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
    "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model.save('Fifth_Attempt/FifthAttempt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfk.models.load_model('Fifth_Attempt/FifthAttempt')\n",
    "# Autoregressive Forecasting\n",
    "reg_predictions = np.array([])\n",
    "X_temp = X_test_reg\n",
    "for reg in range(0,TELESCOPE,AUTOREGRESSIVE_TELESCOPE):\n",
    "    pred_temp = model.predict(X_temp,verbose=0)\n",
    "    if(len(reg_predictions)==0):\n",
    "        reg_predictions = pred_temp\n",
    "    else:\n",
    "        reg_predictions = np.concatenate((reg_predictions,pred_temp),axis=1)\n",
    "    X_temp = np.concatenate((X_temp[:,AUTOREGRESSIVE_TELESCOPE:,:],pred_temp), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the predictions\n",
    "print(f\"Predictions shape: {reg_predictions.shape}\")\n",
    "\n",
    "# Calculate and print Mean Squared Error (MSE)\n",
    "mean_squared_error = tfk.metrics.mean_squared_error(y_test_reg.flatten(), reg_predictions.flatten()).numpy()\n",
    "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
    "\n",
    "# Calculate and print Mean Absolute Error (MAE)\n",
    "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test_reg.flatten(), reg_predictions.flatten()).numpy()\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_multivariate_prediction(X_test_reg, y_test_reg,reg_predictions, TELESCOPE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
